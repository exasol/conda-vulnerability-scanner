import contextlib
import dataclasses
import datetime
import importlib.resources
import json
import logging
import re
import shutil
import sqlite3
import subprocess
import sys
import tempfile
import time
import urllib.request
from argparse import ArgumentParser, Namespace
from collections import defaultdict
from dataclasses import dataclass
from enum import Enum, auto
from gzip import GzipFile
from html.parser import HTMLParser
from io import BytesIO
from pathlib import Path
from sqlite3 import Connection
from stat import S_IRWXU
from typing import List, Dict, Set, Tuple, Callable, Iterator, TextIO, BinaryIO, Optional

DATA_FEED_FILE_EXTENSION = "json.gz"

DATA_FEED_URL_PATH_PREFIX = "/feeds/json/cve/1.1/nvdcve-1.1"

DATA_FEED_FILE_GLOB = "nvdcve-1.1*.json.gz"

CPE_SQLITE_NAME = "cpe.sqlite3"

script_directory = Path(__file__).absolute().parent
base_directory = script_directory.parent


class ScanResult(Enum):
    NO_CVE_FOUND = auto()
    CVE_FOUND = auto()


class Severity(Enum):
    NONE = 0.0
    LOW = 0.1
    MEDIUM = 4.0
    HIGH = 7.0
    CRITICAL = 9.0


@contextlib.contextmanager
def get_go_cpe_dictionary_binary() -> Iterator[Path]:
    yield from make_binary_available("go-cpe-dictionary")


@contextlib.contextmanager
def get_cpe2cve_binary() -> Iterator[Path]:
    yield from make_binary_available("cpe2cve")


def make_binary_available(binary_name: str) -> Iterator[Path]:
    with tempfile.TemporaryDirectory() as tempdir:
        binary_path = Path(tempdir) / binary_name

        with importlib.resources.open_binary("conda_vulnerability_scanner.resources.binaries",
                                             binary_name + ".gz") as res:
            with binary_path.open("w+b") as f:
                unzip_res = GzipFile(fileobj=res)
                shutil.copyfileobj(unzip_res, f)
        binary_path.chmod(S_IRWXU)
        yield binary_path


def retry(function: Callable, retry_count=5, base_wait_seconds=5):
    wait_time = base_wait_seconds
    for attempt in range(retry_count):
        try:
            return function()
        except Exception as e:
            if attempt < retry_count - 1:
                logging.warning(f"Caught exception {e}, retry attempt {attempt + 1} in {wait_time}")
                time.sleep(wait_time)
                wait_time *= base_wait_seconds
            else:
                raise


def read_package_list_file(package_list_file: Path) -> List[Dict[str, str]]:
    with package_list_file.open("r") as f:
        return json.load(f)


def scan_and_report(db_dir_path: Path,
                    json_report_file_path: Path,
                    package_list_file: Path,
                    severity_filter: Severity = Severity.NONE,
                    cve_ignore_list_file: Optional[Path] = None) -> ScanResult:
    cves = scan(db_dir_path, package_list_file, severity_filter, cve_ignore_list_file)
    generate_plaintext_report(cves, sys.stdout)
    write_json_report(cves, json_report_file_path)
    if len(cves) == 0:
        return ScanResult.NO_CVE_FOUND
    else:
        return ScanResult.CVE_FOUND


def scan(db_dir_path: Path, package_list_file: Path,
         severity_filter: Severity = Severity.NONE,
         cve_ignore_list_file: Optional[Path] = None):
    with contextlib.closing(sqlite3.connect(db_dir_path / CPE_SQLITE_NAME)) as connection:
        datafeed_files = list(db_dir_path.glob(DATA_FEED_FILE_GLOB))
        with get_cpe2cve_binary() as cpe2cve:
            packages = read_package_list_file(package_list_file)
            cpes = lookup_cpes_for_packages(packages, connection)
            cves = lookup_cves_for_cpes(cpe2cve, datafeed_files, cpes)
            cves = filter_by_cve_severity(cves, severity_filter)
            if cve_ignore_list_file is not None:
                with open(cve_ignore_list_file) as f:
                    cve_ignore_list = parse_cve_ignore_list(f)
                cves = filter_ignored_cves(cves, cve_ignore_list)
            return cves


def fetch_db(db_dir_path: Path):
    db_dir_path.mkdir(parents=True, exist_ok=True)
    with get_go_cpe_dictionary_binary() as go_cpe_dictionary:
        cpe_sqlite_path = fetch_cpe_dictionary(go_cpe_dictionary, db_dir_path)
        datafeed_files = download_all_nvd_datafeed_files(db_dir_path)
        test_db(db_dir_path)


def test_db(db_dir_path):
    with tempfile.TemporaryDirectory() as temp_dir:
        package_list_file = Path(temp_dir) / "package_list.json"
        with open(package_list_file, "w") as f:
            test_packate_list = [{
                "name": "bzip2",
                "version": "1.0.8"
            }]
            json.dump(test_packate_list, f)
        scan(db_dir_path, package_list_file)


def fetch_cpe_dictionary(cpe_dictionary_bin: Path, output_dir: Path) -> Path:
    db_file_path = output_dir / CPE_SQLITE_NAME
    command = [str(cpe_dictionary_bin), "fetch", "nvd", "--dbpath", str(db_file_path)]
    process = subprocess.check_call(command)
    return db_file_path


def download_file(url: str, file: BinaryIO):
    with urllib.request.urlopen(url) as r:
        if r.status != 200:
            raise RuntimeError(f"Could not download {url}. Got status code {r.status} and reason {r.reason}")
        while chunk := r.read():
            file.write(chunk)


def download_nvd_datafeed() -> str:
    def download():
        f = BytesIO()
        download_file("https://nvd.nist.gov/vuln/data-feeds", f)
        return f.getvalue().decode("utf-8")

    result = retry(download)
    return result


def parse_nvd_datafeed(datafeed: str) -> List[str]:
    class DataFeedHTMLParser(HTMLParser):
        def __init__(self):
            super().__init__()
            self.links = []

        def handle_starttag(self, tag, attrs):
            attrs_dict = dict(attrs)
            if tag == "a" and "href" in attrs_dict \
                    and attrs_dict["href"].endswith(DATA_FEED_FILE_EXTENSION) \
                    and attrs_dict["href"].startswith(DATA_FEED_URL_PATH_PREFIX):
                self.links.append(attrs_dict["href"])

        def handle_endtag(self, tag):
            pass

        def handle_data(self, data):
            pass

    parser = DataFeedHTMLParser()
    parser.feed(datafeed)
    return parser.links


def download_nvd_datafeed_file(link: str, output_dir: Path) -> Path:
    base_url = "https://nvd.nist.gov/"
    url = base_url + link
    file_name = Path(link).name
    file_path = output_dir / file_name
    logging.info(f"Download {url} to {file_path}")

    def download():
        with open(file_path, "wb") as f:
            download_file(url, f)

    retry(download)
    return file_path


def download_all_nvd_datafeed_files(output_dir: Path) -> List[Path]:
    current_year = datetime.datetime.now().year
    links = [f"{DATA_FEED_URL_PATH_PREFIX}-{year}.{DATA_FEED_FILE_EXTENSION}"
             for year in list(range(2002, current_year + 1))]
    return [download_nvd_datafeed_file(link, output_dir) for link in links]


def generate_package_name_variants(package: str):
    yield package
    if package.startswith("lib"):
        yield package[len("lib"):]
    for separator in ["_", "-", " "]:
        replace_separator = package.replace(separator, "\\.")
        yield replace_separator
        if replace_separator.startswith("lib"):
            yield replace_separator[len("lib"):]
    for s in package.split("[-_ ]"):
        yield s
        if s.startswith("lib"):
            yield s[len("lib"):]


def lookup_cpes_for_packages(packages: List[Dict[str, str]], connection: Connection) -> Set[Tuple[str, str, str]]:
    result = set()
    for package in packages:
        for package_name_variant in generate_package_name_variants(package["name"]):
            version = package["version"]
            rows = connection.execute(
                f"""SELECT distinct vendor, product from categorized_cpes where part='a' and (product = '{package_name_variant}')""").fetchall()
            skip_further_variants = False
            for r in rows:
                result.add((r[0], r[1], version))
                skip_further_variants = True
            if skip_further_variants:
                break
    return result


def tuple_to_cpe(cpe_tuple: Tuple[str, str, str]) -> str:
    vendor = cpe_tuple[0]
    product = cpe_tuple[1]
    version = cpe_tuple[2]
    return f"cpe:/a:{vendor}:{product}:{version}"


@dataclass(order=True)
class CVE:
    cve: str
    cvss: str
    cvss2: str
    cvss3: str
    cwe: str
    cpe: str


def parse_cve_ignore_list(fileobj: TextIO) -> List[str]:
    line_without_comment = [line.split("#")[0] for line in fileobj.readlines()]
    strip_lines = [line.strip(" \t\n\r") for line in line_without_comment]
    non_empty_lines = [line for line in strip_lines if line != ""]
    return non_empty_lines


def lookup_cves_for_cpes(cpe2cve_bin: Path, nvd_datafeed_files: List[Path],
                         cpes: Set[Tuple[str, str, str]]) -> List[CVE]:
    if len(nvd_datafeed_files) == 0:
        raise ValueError("No NVD datafeed files found.")
    not_existing_nvd_datafeed_files = [str(file) for file in nvd_datafeed_files if not file.exists()]
    if len(not_existing_nvd_datafeed_files) > 0:
        raise ValueError(f"The following NVD datafeed files {not_existing_nvd_datafeed_files} don't exists.")
    nvd_datafeed_files = [str(file) for file in nvd_datafeed_files]
    command = [str(cpe2cve_bin), "-cpe", "1", "-cve", "1", "-cvss", "2", "-cvss2", "3", "-cvss3", "4", "-cwe",
               "5"] + nvd_datafeed_files
    cpes_str = "\n".join(tuple_to_cpe(cpe) for cpe in cpes)
    stdout = subprocess.check_output(command, input=cpes_str.encode("utf-8"))
    stdout_str = stdout.decode("utf-8")
    lines = stdout_str.splitlines()
    result: List[CVE] = []
    for line in lines:
        split = line.split("\t")
        result.append(CVE(split[0], split[1], split[2], split[3], split[4], split[5]))
    return result


def filter_ignored_cves(cves: List[CVE], cve_ignore_list: List[str]):
    cve_ignore_set = set(cve_ignore_list)
    return [cve for cve in cves if cve.cve not in cve_ignore_set]


def filter_by_cve_severity(cves: List[CVE], severity: Severity) -> List[CVE]:
    float_pattern = re.compile("[0-9]+.[0-9]+")
    filtered_cve = [cve for cve in cves
                    if not float_pattern.match(cve.cvss)
                    or float(cve.cvss) >= severity.value]
    return filtered_cve


def write_json_report(cves: List[CVE], output_file: Path):
    with output_file.open("w") as f:
        dicts = [dataclasses.asdict(cve) for cve in cves]
        json.dump(dicts, f)


def generate_plaintext_report(cves: List[CVE], file: TextIO):
    column_width = defaultdict(lambda: 0)
    header = ("cpe", "cve", "cvss", "cvss2", "cvss3", "cwe", "url")
    cves = [(cve.cpe, cve.cve, cve.cvss, cve.cvss2, cve.cvss3, cve.cwe, "https://nvd.nist.gov/vuln/detail/" + cve.cve)
            for cve in cves]
    for row in [header] + cves:
        for index, column in enumerate(row):
            column_width[index] = max(column_width[index], len(column))
    separator_line = generate_separator_line(column_width, header)
    print(separator_line, file=file)
    header_line = generate_header_line(column_width, header)
    print(header_line, file=file)
    print(separator_line, file=file)
    for row in cves:
        line = "| ".join(
            column.ljust(column_width[index] + 1, ' ')
            for index, column in enumerate(row))
        print(line, file=file)


def generate_header_line(column_width, header):
    header_line = "| ".join(
        column.ljust(column_width[index] + 1, ' ')
        for index, column in enumerate(header))
    return header_line


def generate_separator_line(column_width, header):
    separator_line = "| ".join(
        "".ljust(column_width[index] + 1, '_')
        for index, column in enumerate(header))
    return separator_line


class ExitCode(Enum):
    SUCCESS = 0
    ERROR = 1
    COMMAND_LINE_PARSER_ERROR = 2
    CVE_FOUND = 3


def main(args: List[str],
         scan_and_report: Callable[[Path, Path, Path, Severity, Optional[Path]], ScanResult],
         fetch_db: Callable[[Path], None]) -> int:
    try:
        return main_debug(args, scan_and_report, fetch_db)
    except Exception as e:
        logging.error(f"Caught error: {e}")
        return ExitCode.ERROR.value


def main_debug(args: List[str],
               scan_and_report: Callable[[Path, Path, Path, Severity, Optional[Path]], ScanResult],
               fetch_db: Callable[[Path], None]) -> int:
    parser = generate_argument_parser()
    args = parser.parse_args(args)
    if args.command == "scan":
        return handle_scan_command(args, scan_and_report)
    elif args.command == "fetch":
        db_directory = Path(args.db_directory)
        fetch_db(db_directory)
        return ExitCode.SUCCESS.value
    else:
        logging.error(f"Unknown command '{args.command}'")
        return ExitCode.ERROR.value


def handle_scan_command(
        args: Namespace,
        scan_and_report: Callable[[Path, Path, Path, Severity, Optional[Path]], ScanResult]) -> int:
    db_directory = Path(args.db_directory)
    if not db_directory.is_dir():
        print(f"DB Directory {db_directory} not found", file=sys.stderr)
        return ExitCode.ERROR.value
    package_list_file = Path(args.package_list_file)
    if not package_list_file.is_file():
        print(f"Package list file '{package_list_file}' not found", file=sys.stderr)
        return ExitCode.ERROR.value
    json_report_file = Path(args.json_report_file)
    cve_ignore_list_file = None
    if args.cve_ignore_list_file is not None:
        cve_ignore_list_file = Path(args.cve_ignore_list_file)
        if not cve_ignore_list_file.is_file():
            print(f"CVE ignore list file '{cve_ignore_list_file}' not found", file=sys.stderr)
            return ExitCode.ERROR.value
    if args.severity_filter is None:
        severity_filter = Severity.NONE
    else:
        severity_filter = Severity[args.severity_filter]
    scan_result = scan_and_report(db_directory, json_report_file, package_list_file,
                                  severity_filter, cve_ignore_list_file)
    if scan_result == ScanResult.CVE_FOUND:
        return ExitCode.CVE_FOUND.value
    else:
        return ExitCode.SUCCESS.value


def generate_argument_parser() -> ArgumentParser:
    parser = ArgumentParser()
    subparsers = parser.add_subparsers(dest='command')
    scan_parser = subparsers.add_parser('scan')
    scan_parser.add_argument('--db-directory', type=str, required=True,
                             help="Directory where the DB is stored.")
    scan_parser.add_argument('--json-report-file', type=str, required=True,
                             help="File the json report gets stored.")
    scan_parser.add_argument('--package-list-file', type=str, required=True,
                             help="File which contains the output of 'conda list --json'.")
    scan_parser.add_argument('--cve-ignore-list-file', type=str, required=False,
                             help="File which contains in each line a CVE number "
                                  "which gets ignored during the scan."
                                  "Empty lines or lines with whitespace only are ignored. "
                                  "Also every text which starts with # gets ignored.")
    scan_parser.add_argument('--severity-filter', type=str,
                             choices=list(severity.name for severity in Severity),
                             default=Severity.NONE.name,
                             help="Filters all CVEs with a lower severity from the result.")

    fetch_parser = subparsers.add_parser('fetch')
    fetch_parser.add_argument('--db-directory', type=str, required=True,
                              help="Directory where the DB gets stored.")
    return parser


if __name__ == '__main__':
    logging.basicConfig(stream=sys.stderr, level=logging.INFO)
    sys.exit(main(sys.argv[1:], scan_and_report, fetch_db))
