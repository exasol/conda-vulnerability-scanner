import contextlib
import dataclasses
import datetime
import importlib.resources
import json
import logging
import shutil
import sqlite3
import subprocess
import sys
import tempfile
import time
import urllib.request
from argparse import ArgumentParser
from collections import defaultdict
from dataclasses import dataclass
from enum import Enum, auto
from gzip import GzipFile
from html.parser import HTMLParser
from io import BytesIO
from pathlib import Path
from sqlite3 import Connection
from stat import S_IRWXU
from typing import List, Dict, Set, Tuple, Callable, Iterator, TextIO, BinaryIO

DATA_FEED_FILE_EXTENSION = "json.gz"

DATA_FEED_URL_PATH_PREFIX = "/feeds/json/cve/1.1/nvdcve-1.1"

DATA_FEED_FILE_GLOB = "nvdcve-1.1*.json.gz"

CPE_SQLITE_NAME = "cpe.sqlite3"

script_directory = Path(__file__).absolute().parent
base_directory = script_directory.parent


@contextlib.contextmanager
def get_go_cpe_dictionary_binary() -> Iterator[Path]:
    yield from make_binary_available("go-cpe-dictionary")


@contextlib.contextmanager
def get_cpe2cve_binary() -> Iterator[Path]:
    yield from make_binary_available("cpe2cve")


def make_binary_available(binary_name: str) -> Iterator[Path]:
    with tempfile.TemporaryDirectory() as tempdir:
        binary_path = Path(tempdir) / binary_name

        with importlib.resources.open_binary("conda_security_scanner.resources.binaries", binary_name + ".gz") as res:
            with binary_path.open("w+b") as f:
                unzip_res = GzipFile(fileobj=res)
                shutil.copyfileobj(unzip_res, f)
        binary_path.chmod(S_IRWXU)
        yield binary_path


def retry(function: Callable, retry_count=5, base_wait_seconds=5):
    wait_time = base_wait_seconds
    for attempt in range(retry_count):
        try:
            return function()
        except Exception as e:
            if attempt < retry_count - 1:
                logging.warning(f"Caught exception {e}, retry attempt {attempt + 1} in {wait_time}")
                time.sleep(wait_time)
                wait_time *= base_wait_seconds
            else:
                raise


def read_package_list_file(package_list_file: Path) -> List[Dict[str, str]]:
    with package_list_file.open("r") as f:
        return json.load(f)


class ScanResult(Enum):
    NO_CVE_FOUND = auto()
    CVE_FOUND = auto()


def scan_and_report(db_dir_path: Path, json_report_file_path: Path, package_list_file: Path) -> ScanResult:
    cves = scan(db_dir_path, package_list_file)
    generate_plaintext_report(cves, sys.stdout)
    write_json_report(cves, json_report_file_path)
    if len(cves) == 0:
        return ScanResult.NO_CVE_FOUND
    else:
        return ScanResult.CVE_FOUND


def scan(db_dir_path, package_list_file):
    with contextlib.closing(sqlite3.connect(db_dir_path / CPE_SQLITE_NAME)) as connection:
        datafeed_files = list(db_dir_path.glob(DATA_FEED_FILE_GLOB))
        with get_cpe2cve_binary() as cpe2cve:
            packages = read_package_list_file(package_list_file)
            cpes = lookup_cpes_for_packages(packages, connection)
            cves = lookup_cves_for_cpes(cpe2cve, datafeed_files, cpes)
            return cves


def fetch_db(db_dir_path: Path):
    db_dir_path.mkdir(parents=True, exist_ok=True)
    with get_go_cpe_dictionary_binary() as go_cpe_dictionary:
        cpe_sqlite_path = fetch_cpe_dictionary(go_cpe_dictionary, db_dir_path)
        datafeed_files = download_all_nvd_datafeed_files(db_dir_path)
        test_db(db_dir_path)


def test_db(db_dir_path):
    with tempfile.TemporaryDirectory() as temp_dir:
        package_list_file = Path(temp_dir) / "package_list.json"
        with open(package_list_file, "w") as f:
            test_packate_list = [{
                "name": "bzip2",
                "version": "1.0.8"
            }]
            json.dump(test_packate_list, f)
        scan(db_dir_path, package_list_file)


def fetch_cpe_dictionary(cpe_dictionary_bin: Path, output_dir: Path) -> Path:
    db_path = output_dir / CPE_SQLITE_NAME
    command = [str(cpe_dictionary_bin), "fetch", "nvd", "--dbpath", str(db_path)]
    process = subprocess.check_call(command, cwd=output_dir)
    return db_path


def download_file(url: str, file: BinaryIO):
    with urllib.request.urlopen(url) as r:
        if r.status != 200:
            raise RuntimeError(f"Could not download {url}. Got status code {r.status} and reason {r.reason}")
        while chunk := r.read():
            file.write(chunk)


def download_nvd_datafeed() -> str:
    def download():
        f = BytesIO()
        download_file("https://nvd.nist.gov/vuln/data-feeds", f)
        return f.getvalue().decode("utf-8")

    result = retry(download)
    return result


def parse_nvd_datafeed(datafeed: str) -> List[str]:
    class DataFeedHTMLParser(HTMLParser):
        def __init__(self):
            super().__init__()
            self.links = []

        def handle_starttag(self, tag, attrs):
            attrs_dict = dict(attrs)
            if tag == "a" and "href" in attrs_dict \
                    and attrs_dict["href"].endswith(DATA_FEED_FILE_EXTENSION) \
                    and attrs_dict["href"].startswith(DATA_FEED_URL_PATH_PREFIX):
                self.links.append(attrs_dict["href"])

        def handle_endtag(self, tag):
            pass

        def handle_data(self, data):
            pass

    parser = DataFeedHTMLParser()
    parser.feed(datafeed)
    return parser.links


def download_nvd_datafeed_file(link: str, output_dir: Path) -> Path:
    base_url = "https://nvd.nist.gov/"
    url = base_url + link
    file_name = Path(link).name
    file_path = output_dir / file_name
    logging.info(f"Download {url} to {file_path}")

    def download():
        with open(file_path, "wb") as f:
            download_file(url, f)

    retry(download)
    return file_path


def download_all_nvd_datafeed_files(output_dir: Path) -> List[Path]:
    current_year = datetime.datetime.now().year
    links = [f"{DATA_FEED_URL_PATH_PREFIX}-{year}.{DATA_FEED_FILE_EXTENSION}"
             for year in list(range(2002, current_year + 1))]
    return [download_nvd_datafeed_file(link, output_dir) for link in links]


def generate_package_name_variants(package: str):
    yield package
    if package.startswith("lib"):
        yield package[len("lib"):]
    for separator in ["_", "-", " "]:
        replace_separator = package.replace(separator, "\\.")
        yield replace_separator
        if replace_separator.startswith("lib"):
            yield replace_separator[len("lib"):]
    for s in package.split("[-_ ]"):
        yield s
        if s.startswith("lib"):
            yield s[len("lib"):]


def lookup_cpes_for_packages(packages: List[Dict[str, str]], connection: Connection) -> Set[Tuple[str, str, str]]:
    result = set()
    for package in packages:
        for package_name_variant in generate_package_name_variants(package["name"]):
            version = package["version"]
            rows = connection.execute(
                f"""SELECT distinct vendor, product from categorized_cpes where part='a' and (product = '{package_name_variant}')""").fetchall()
            skip_further_variants = False
            for r in rows:
                result.add((r[0], r[1], version))
                skip_further_variants = True
            if skip_further_variants:
                break
    return result


def tuple_to_cpe(cpe_tuple: Tuple[str, str, str]) -> str:
    vendor = cpe_tuple[0]
    product = cpe_tuple[1]
    version = cpe_tuple[2]
    return f"cpe:/a:{vendor}:{product}:{version}"


@dataclass(order=True)
class CVE:
    cve: str
    cvss: str
    cvss2: str
    cvss3: str
    cwe: str
    cpe: str


def lookup_cves_for_cpes(cpe2cve_bin: Path, nvd_datafeed_files: List[Path],
                         cpes: Set[Tuple[str, str, str]]) -> List[CVE]:
    if len(nvd_datafeed_files) == 0:
        raise ValueError("No NVD datafeed files found.")
    not_existing_nvd_datafeed_files = [str(file) for file in nvd_datafeed_files if not file.exists()]
    if len(not_existing_nvd_datafeed_files) > 0:
        raise ValueError(f"The following NVD datafeed files {not_existing_nvd_datafeed_files} don't exists.")
    nvd_datafeed_files = [str(file) for file in nvd_datafeed_files]
    command = [str(cpe2cve_bin), "-cpe", "1", "-cve", "1", "-cvss", "2", "-cvss2", "3", "-cvss3", "4", "-cwe",
               "5"] + nvd_datafeed_files
    cpes_str = "\n".join(tuple_to_cpe(cpe) for cpe in cpes)
    stdout = subprocess.check_output(command, input=cpes_str.encode("utf-8"))
    stdout_str = stdout.decode("utf-8")
    lines = stdout_str.splitlines()
    result: List[CVE] = []
    for line in lines:
        split = line.split("\t")
        result.append(CVE(split[0], split[1], split[2], split[3], split[4], split[5]))
    return result


def write_json_report(cves: List[CVE], output_file: Path):
    with output_file.open("w") as f:
        dicts = [dataclasses.asdict(cve) for cve in cves]
        json.dump(dicts, f)


def generate_plaintext_report(cves: List[CVE], file: TextIO):
    column_width = defaultdict(lambda: 0)
    header = ("cpe", "cve", "cvss", "cvss2", "cvss3", "cwe", "url")
    cves = [(cve.cpe, cve.cve, cve.cvss, cve.cvss2, cve.cvss3, cve.cwe, "https://nvd.nist.gov/vuln/detail/" + cve.cve)
            for cve in cves]
    for row in [header] + cves:
        for index, column in enumerate(row):
            column_width[index] = max(column_width[index], len(column))
    separator_line = generate_separator_line(column_width, header)
    print(separator_line, file=file)
    header_line = generate_header_line(column_width, header)
    print(header_line, file=file)
    print(separator_line, file=file)
    for row in cves:
        line = "| ".join(
            column.ljust(column_width[index] + 1, ' ')
            for index, column in enumerate(row))
        print(line, file=file)


def generate_header_line(column_width, header):
    header_line = "| ".join(
        column.ljust(column_width[index] + 1, ' ')
        for index, column in enumerate(header))
    return header_line


def generate_separator_line(column_width, header):
    separator_line = "| ".join(
        "".ljust(column_width[index] + 1, '_')
        for index, column in enumerate(header))
    return separator_line


class ExitCode(Enum):
    SUCCESS = 0
    ERROR = 1
    COMMAND_LINE_PARSER_ERROR = 2
    CVE_FOUND = 3


def main(args: List[str],
         scan_and_report: Callable[[Path, Path, Path], ScanResult],
         fetch_db: Callable[[Path], None]) -> int:
    try:
        parser = generate_argument_parser()
        args = parser.parse_args(args)
        if args.command == "scan":
            return handle_scan_command(args, scan_and_report)
        elif args.command == "fetch":
            db_directory = Path(args.db_directory)
            fetch_db(db_directory)
            return ExitCode.SUCCESS.value
        else:
            logging.error(f"Unknown command '{args.command}'")
            return ExitCode.ERROR.value
    except Exception as e:
        logging.error("Caught error", e)
        return ExitCode.ERROR.value


def handle_scan_command(args, scan_and_report):
    db_directory = Path(args.db_directory)
    if not db_directory.is_dir():
        print(f"DB Directory {db_directory} not found", file=sys.stderr)
        return ExitCode.ERROR.value
    package_list_file = Path(args.package_list_file)
    if not package_list_file.is_file():
        print(f"Package list file '{package_list_file}' not found", file=sys.stderr)
        return ExitCode.ERROR.value
    json_report_file = Path(args.json_report_file)
    scan_result = scan_and_report(db_directory, json_report_file, package_list_file)
    if scan_result == ScanResult.CVE_FOUND:
        return ExitCode.CVE_FOUND.value
    else:
        return ExitCode.SUCCESS.value


def generate_argument_parser() -> ArgumentParser:
    parser = ArgumentParser()
    subparsers = parser.add_subparsers(dest='command')
    scan_parser = subparsers.add_parser('scan')
    scan_parser.add_argument('--db-directory', type=str, required=True,
                             help="Directory where the DB is stored.")
    scan_parser.add_argument('--json-report-file', type=str, required=True,
                             help="File the json report gets stored.")
    scan_parser.add_argument('--package-list-file', type=str, required=True,
                             help="File which contains the output of 'conda list --json'.")
    fetch_parser = subparsers.add_parser('fetch')
    fetch_parser.add_argument('--db-directory', type=str, required=True,
                              help="Directory where the DB gets stored.")
    return parser


if __name__ == '__main__':
    sys.exit(main(sys.argv[1:], scan_and_report, fetch_db))
